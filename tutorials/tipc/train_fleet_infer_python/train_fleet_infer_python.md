# Linux GPU 多机多卡训练推理测试开发文档

# 目录

- [1. 简介](#1)
  
  近十年来，深度学习技术不断刷新视觉、自然语言处理、语音、搜索和推荐等领域任务的记录。这其中的原因，用一个关键词描述就是“大规模”。大规模的数据使得模型有足够的知识可以记忆，大规模参数量的模型使得模型本身有能力记忆更多的数据，大规模高性能的算力（以GPU为典型代表）使得模型的训练速度有百倍甚至千倍的提升。大规模的数据、模型和算力作为深度学习技术的基石，在推动深度学习技术发展的同时，也给深度学习训练带来了新的挑战：大规模数据和大规模模型的发展使得深度学习模型的能力不断增强，要求我们更加合理地利用大规模集群算力进行高效地训练，这是分布式训练面临的主要挑战。
  
  飞桨分布式从产业实践出发，提供包括数据并行、模型并行和流水线并行等在内的完备的并行能力，提供简单易用地分布式训练接口和丰富的底层通信原语，赋能用户业务发展。
  
  本文，我们以最常用的数据并行为例，介绍LInux GPU多机多卡从训练到推理的使用。
- [2. 多机多卡训练功能开发](#2---)
  
  数据并行(data parallelism)主要逻辑遵循[Single Program Multiple Data](https://en.wikipedia.org/wiki/SPMD)的原则，即在数据并行的模型训练中，训练任务被切分到多个进程(设备)上,每个进程维护相同的模型参数和相同的计算任务，但是处理不同的数据(batch data)。通过这种方式，同一全局数据(global batch)下的数据和计算被切分到了不同的进程，从而减轻了单个设备上的计算和存储压力。
  
  在深度学习模型训练中，数据并行可作为通过增加并行训练设备来提高训练吞吐量(global batch size per second) 的方法。以常见的ResNet50 模型使用32GB V100卡训练为例。假设训练时单卡最大能支持的local batch size为256，训练一个step的耗时为1秒。则单卡训练时的吞吐为256 imgs/s。如果我们使用32 张V100 做数据并行训练，假设没有损耗，那么理论上的训练吞吐可达到 32 x 256 = 8192 imgs/。实际上由于数据并行时多机多卡的通信消耗等，实际加速效率会有折扣，但在加速效率为0.8时，训练吞吐也可达到32 x 256 x 0.8 = 6554 imgs/s。如果使用更多的GPU，并行训练的速度将会更高，大大减少训练需要的时间。
  
  与单机单卡的普通模型训练相比，使用飞桨分布式训练的代码都只需要补充三个部分代码：
  1. 导入分布式训练需要的依赖包。
  
  2. 初始化分布式环境。
  
  3. 设置分布式训练需要的优化器。
  
  下面将逐一进行讲解。
  
  - 2.1 导入依赖
    
    导入必要的依赖，例如分布式训练专用的Fleet API(paddle.distributed.fleet)。
    
    ```python
    from paddle.distributed import fleet
    ```
  
  - 2.2 初始化分布式环境
    
    包括定义缺省的分布式策略，然后通过将参数is_collective设置为True。
    
    ```python
    strategy = fleet.DistributedStrategy()
    fleet.init(is_collective=True, strategy=strategy)
    ```
  
  - 2.3 设置分布式训练需要的优化器
    
    使用paddle.DataParallel封装模型
    
    ```python
    model = paddle.DataParallel(model)
    ```
- [3. 多机多卡推理功能开发](#3---)
  
  由于数据并行训练各个卡上包含完整的模型副本，因此只需要保存某张卡上的模型用于推理即可。通常，可以选择保存第一张卡上的模型用于推理。
  
  ```python
  if fleet.worker_index() == 0:
      # save inference model
  ```
  
  
- [4. FAQ](#4)
